######METAGENOMIC SEQUENCING: SHORT-READ ASSEMBLY, MAPPING AND MAG RECONSTRUCTION USING ANVI'O#####
###The workflow and code used, except blasts against specific databases, are all part of anvi'o (see the official website for more information): https://anvio.org/ 


###Before starting: put fastq pair-end read files in a folder named 00_RAW and a create a .txt file that contains sample names in the first column, the pathway to R1 files in the second column and the pathway to R2 files in the third column.

conda activate anvio-8

###Step 1: quality control of short-reads using the criteria described by Minoche et al.
mkdir 01_QC
iu-gen-configs samples.txt -o 01_QC
ls 01_QC/
Sample_01.ini
Sample_02.ini
for ini in 01_QC/*.ini; do iu-filter-quality-minoche $ini; done

###Step 2: metagenomic short-read assembly using MEGAHIT
cd 01_QC
cat *R1.fastq > R1.fastq
cat *R2.fastq > R2.fastq
gzip R*.fastq
megahit -1 R1.fastq.gz -2 R2.fastq.gz --min-contig-len 1000 -o /home/pers/concepcion.sanchez-c/temperature/DNA/megahit -t 16 --k-list 21,41,61,81,99
cd ../
cd megahit  
anvi-script-reformat-fasta final.contigs.fa -o contigs.fa --simplify-names --report-file rename.txt

####Step 3: mapping of short reads onto contigs using Bowtie2
cd ../
NUM_THREADS=16
names='D0_1 D0_2 D0_3 D0_4 D0_6 D14_20_1 D14_20_2 D14_20_3 D14_28_1 D14_28_2 D14_28_3 D14_VAR_1 D14_VAR_2 D14_VAR_3 D28_20_1 D28_20_2 D28_20_3 D28_28_1 D28_28_2 D28_28_3 D28_VAR_1 D28_VAR_2 D28_VAR_3'
mkdir 04_MAPPING
bowtie2-build megahit/contigs.fa 04_MAPPING/contigs

for sample in $names
do
bowtie2 --threads $NUM_THREADS -x 04_MAPPING/contigs -1 /home/pers/concepcion.sanchez-c/temperature/DNA/01_QC/${sample}-QUALITY_PASSED_R1.fastq -2 /home/pers/concepcion.sanchez-c/temperature/DNA/01_QC/${sample}-QUALITY_PASSED_R2.fastq --no-unal -S 04_MAPPING/$sample.sam
samtools view -F 4 -bS 04_MAPPING/$sample.sam > 04_MAPPING/$sample-RAW.bam
anvi-init-bam 04_MAPPING/$sample-RAW.bam -o 04_MAPPING/$sample.bam
rm 04_MAPPING/$sample.sam 04_MAPPING/$sample-RAW.bam
done

###Step 4: create a contig database from the contig file
anvi-gen-contigs-database -f /home/pers/concepcion.sanchez-c/temperature/DNA/megahit/contigs.fa -o contigs.db -n 'Temp_MG'

#####Hidden Markov models: utilize multiple default bacterial single-copy core gene collections and identify hits among your genes to those collections using HMMER
anvi-run-hmms -c contigs.db --num-threads 16

####Annotate genes in your contigs database with functions from the NCBIâ€™s Clusters of Orthologus Groups, EBI's pfam database and (previously downloaded)
anvi-setup-ncbi-cogs --cog-version COG20 --cog-data-dir COG_2020 
#anvi-setup-pfams --pfam-version 32.0 --pfam-data-dir Pfam_v32
anvi-run-ncbi-cogs -c contigs.db --cog-data-dir COG_2020 --num-threads 16
anvi-run-pfams -c contigs.db --pfam-data-dir Pfam_v32 --num-threads 16
anvi-run-scg-taxonomy -c contigs.db


####Step 5: Create individual profiles for each sample
names='D0_1 D0_2 D0_3 D0_4 D0_6 D14_20_1 D14_20_2 D14_20_3 D14_28_1 D14_28_2 D14_28_3 D14_VAR_1 D14_VAR_2 D14_VAR_3 D28_20_1 D28_20_2 D28_20_3 D28_28_1 D28_28_2 D28_28_3 D28_VAR_1 D28_VAR_2 D28_VAR_3'
mkdir 05_PROFILES
for sample in $names ; do anvi-profile -c contigs.db -i 04_MAPPING/$sample.bam --sample-name $sample --min-contig-length 1000 --output-dir 05_PROFILES/$sample ; done 

###Step 6: Merge profiles
mv contigs.db 05_PROFILES
cd 05_PROFILES
anvi-merge */PROFILE.db -o SAMPLES-MERGED -c contigs.db --enforce-hierarchical-clustering

###Contig visualization on anvi'o

anvi-interactive -p PROFILE.db -c contigs.db

###Step 7: Binning, MAG reconstruction and refinement. Binning is done manually based on differential. Bins are stored manually using the anvi'o interactive platform (for more information refer to the anvi'o website)
###Bin refinement on the interactive platform to satisfy quality criteria (completion >50% redundancy <10%) (-C name of the bin collection, -b name of the bin):
anvi-refine -p PROFILE.db -c contigs.db -C bins -b Bin_1
###Refined bins are stored manually based on differential coverage and sequence composition using the interactive platform and summarized using:
anvi-summarize -c contigs.db -p PROFILE.db -C bins

source deactivate anvio_env

##The summary generated contains all the relevant information (MAG abundance, completion, redundancy, functional traits annotated with COG and pfam...) that was used in this study. Plots were obtained using GraphPad Prism 9

####Step 7: Annotation of genes of interest in the MAGs: 
#### 7.1 Diamond blast against specific databases (CARD for antibiotic resistance genes, BacMet for metal resistance genes)
###Example for Rhodoferax MAG and the CARD database, the same was done for all bins and databases, previously downloaded into the folder (databases created using diamond makedb)###

###Blast the contig sequences against the database and filter based on aminoacid identity (>60%) and alignment length (>100 aa) - choose best-hit

diamond blastx --db /databases/card.dmnd --query Rhodoferax-contigs.fa --evalue 0.00001 --out /card/card_contigsRhodoferax.txt -p 16 
awk '$3>=60 && $4>=100 {print;} card_contigsRhodoferax.txt > filtered_cardRhodoferax.txt
awk '!x[$1]++' filtered_cardRhodoferax.txt > besthit_cardRhodoferax.txt

########METAGENOMIC SEQUENCING: ARG AND MRG SCREENING IN NON-ASSEMBLED READS######
####Start from quality-filtered reads (step 1 in the previous code)
####Step 2: conversion to fasta
cd 01_QC
for function in *.fastq; do
fastx_toolkit/fastq_to_fasta -i $function -o $function.fasta -Q33
done
mkdir Fasta_File
mv *.fasta Fasta_File

###Step 3: concatenation of forward and reverse reads (without merging)
cd Fasta_File
ls *.fasta > sample_list.txt

while read ligne
do
	sed 's/\(.*\)\(_R\).*/\1/' > liste_names.txt
done < sample_list.txt

nl liste_names.txt | sort --key 2 --unique | cut --fields 2 > liste_unique_names.txt

while read name
do
	cat $name'_R1'.fastq.fasta  $name'_R2'.fastq.fasta > ./$name'concatenated'.fasta
done < liste_noms_uniques.txt

mkdir concatenated
mv concatenated* concatenated

####Step 4: blast against the CARD database (BacMet database was bacmet.dmnd)
for file in *.fasta
do
diamond blastx --db /databases/card.dmnd --query $file --evalue 0.00001 --out /thermal/card_$file -p 16 
done

mkdir card_results
mv card* card_results
cd card_results

for file in *.fasta
do
awk '$3>=60 && $4>=100 {print;} $file > filtered_$file
done

mkdir filtered
mv filtered* filtered
cd filtered

for file in *.fasta
do
awk '!x[$1]++' $file > besthit_$file

####Step 5: abundance analysis using R

###Input: blast results (filtered) in .tsv format (one file per sample). Two columns: gene (gene names and accession numbers) and count (number of hits) Edit on excel as the example:
###"gene"	"count"
###"1"	"gb|AAA25550.1|ARO:3003105|dfrA3"	1
###"2"	"gb|AAA26793|ARO:3003748|oleC"	1

library(dplyr)
library(tidyverse)

###Create tables with unique gene columns per sample (do it individually for each sample):
setwd("CARD")
arg<-read.table(file ="noMPAB1.txt", header = TRUE, sep = "\t")
arg2 <- arg %>% group_by(gene)
arg3 <- arg2 %>% summarise(n = n())
colnames(arg3)<-colnames(arg)
write.table(arg3, "noMPAB2.tsv", sep ="\t")

setwd("../")
filenames<- list.files("CARD", pattern = "*.tsv", full.names=TRUE)
ldf<-lapply(filenames, read.delim, fill=TRUE, header=TRUE)
names(ldf)=str_remove(filenames,"^.*/")
ldf2<-imap_dfr(ldf, ~cbind(.x,sample=.y))
tab2<-pivot_wider(ldf2, names_from=sample, values_from=count, values_fill=0)
write.table(tab2, "arg_counts.txt")

###Gene counts were normalized per sequencing depth and plotted using GraphPad 9


###########################Clustered heatmap########
###Starting point: MAG abundance table obtained from anvi'o (percent of recruitment)

repart <- read.table("mags.txt",head=TRUE, row.names=1)
repart<-t(repart)
scaleyellowred <- paletteer_c("ggthemes::Classic Orange-White-Blue", 10, -1)
pheatmap(mat = repart, clustering_distance_rows="euclidean", clustering_distance_columns="euclidean", treeheight_row = 30, treeheight_col = 30, fontsize_row = 16, fontsize_col = 16,fontsize = 8, scale="column", border_color="black", cutree_rows=2, cutree_cols=2, color = scaleyellowred)


############NMDS and PERMANOVA###########
library(readr)
relab <- read_table("mags.txt")
metadata <- read_table("metadata.txt")  ###metadata include sampling day, condition and replicate
library(vegan)
NMDS <- metaMDS(relab,distance = "bray")
NMDS
NMDS[["points"]]

#Permanova test
adonis2(relab ~ temperature*time, data = metadata)


####Normality tests and pairwise statistical analyses############
# Performe multiple Shapiro tests
thi=sul

A <- lapply(1:length(thi), function(col) {
      ref = thi[[col]]
      unlist(lapply(thi, function(i) shapiro.test(x = i)$p.value))
    })
  A
  
  # Create a table from the A list
  p_values_table <- do.call(rbind, A)
  # Column names
  comparaisons <- c("RTcontrolsT0", "RTcontrolsT14", "RTcontrolsT28", "ConstantheatstressT14", "ConstantheatstressT28", "RepeatedheatstressT14", "RepeatedheatstressT28")
  # Give column names 

  colnames(p_values_table) <- comparaisons
  # Define the names of the conditions tested
  conditions <- c("RTcontrolsT0", "RTcontrolsT14", "RTcontrolsT28", "ConstantheatstressT14", "ConstantheatstressT28", "RepeatedheatstressT14", "RepeatedheatstressT28")
  
  # Replace the line numbers with the conditions
  rownames(p_values_table) <- conditions
  
  # Display the table with the conditions as row names
  p_values_table
  
  # Display the table of p-values
  p_values_table <- as.data.frame(p_values_table)
  p_values_table
  
  # Define a threshold for significance (here 0.05)
  threshold <- 0.05
  # Round p-values to 4 digits
  p_values_table_rounded <- round(p_values_table, 4)
  
  # Add asterisk levels according to p-value (after rounding)
  p_values_signif <- apply(p_values_table_rounded, 2, function(x) {
    ifelse(x < 0.001, paste0(x, "***"), 
           ifelse(x < 0.01, paste0(x, "**"),
                  ifelse(x < 0.05, paste0(x, "*"), x)))
  })
  
  # Convert to data frame for correct display
  p_values_signif <- as.data.frame(p_values_signif)
  
  # Display the final table with rounded p-values and asterisks
  p_values_signif
  
  # Creating an interactive board with colors
  # Load DT package
  library(DT)
  
  # Round p-values to 4 digits
  p_values_table_rounded <- round(p_values_table, 4)
  
 
  datatable(p_values_table_rounded, 
            options = list(pageLength = 10)) %>%
    formatStyle(names(p_values_table_rounded), 
                backgroundColor = styleInterval(c(0.001, 0.01, 0.05), c('darkred', 'red', 'orange', 'white')))

  
  
  # Performed multiple one sided student  tests 
  B <- lapply(1:length(thi), function(col) {
    ref = thi[[col]]
    unlist(lapply(thi, function(i) t.test(x = i, y = ref,alternative = "greater")$p.value))
  })
  B
  
  # Performed multiple one sided mann whitney tests 
  B <- lapply(1:length(thi), function(col) {
    ref = thi[[col]]
    unlist(lapply(thi, function(i) wilcox.test(x = i, y = ref,alternative = "greater")$p.value))
  })
  B
  
  # Create a table from the A list
  p_values_table <- do.call(rbind, B)
  
  # Column names
  comparaisons <- c("RTcontrolsT0", "RTcontrolsT14", "RTcontrolsT28", "ConstantheatstressT14", "ConstantheatstressT28", "RepeatedheatstressT14", "RepeatedheatstressT28")
  # Give column names
  
  colnames(p_values_table) <- comparaisons
  # Define test condition names
  conditions <- c("RTcontrolsT0", "RTcontrolsT14", "RTcontrolsT28", "ConstantheatstressT14", "ConstantheatstressT28", "RepeatedheatstressT14", "RepeatedheatstressT28")
# Replace line numbers with conditions

rownames(p_values_table) <- conditions
  
# Display table with conditions as row names
  p_values_table
  
# Display the p-values table
  p_values_table <- as.data.frame(p_values_table)
  p_values_table
  
# Define a threshold for significance (here 0.05)
  threshold <- 0.05
  # Round p-values to 4 digits
  p_values_table_rounded <- round(p_values_table, 4)
  
  # Add asterisk levels according to p-value (after rounding)
  p_values_signif <- apply(p_values_table_rounded, 2, function(x) {
    ifelse(x < 0.001, paste0(x, "***"), 
           ifelse(x < 0.01, paste0(x, "**"),
                  ifelse(x < 0.05, paste0(x, "*"), x)))
  })
  
# Convert to data frame for correct display
  p_values_signif <- as.data.frame(p_values_signif)
  
# Display final table with rounded p-values and asterisks
  p_values_signif
  
  # Creating an interactive board with colors
# Load DT package
  library(DT)
  
  # Round p-values to 4 digits
  p_values_table_rounded <- round(p_values_table, 4)
  
 
  datatable(p_values_table_rounded, 
            options = list(pageLength = 10)) %>%
    formatStyle(names(p_values_table_rounded), 
                backgroundColor = styleInterval(c(0.001, 0.01, 0.05), c('darkred', 'red', 'orange', 'white')))
  #Indicate file location
  setwd("C:/Users/....")

#Import file. Depending on your Windows language, you may need to use the read.csv or read.csv2 file.

pang1 <- read.csv("C:/Users/.....pang1.csv"

#Check the normality of the data first => Multiple Shapiro test.

thi=read.csv2("pang1.csv")
A <- lapply(1:length(thi), function(col) {
  ref = thi[[col]]
  unlist(lapply(thi, function(i) shapiro.test(x = i)$p.value))
})
A

#If all data follow a normal distribution =>Tukey(HSD) with the agricolae package => parametric test

library(agricolae)

#Load dataset if not already done

thi=read.csv2("panag1.csv")

#Anova test (aov). Be careful with column names (uppercase, "s" at the end, etc)

ANAV <- aov(Valeurs~Condition, data=thi)
ANAV
anova_result <- aov(Valeurs ~ Conditions, data = thi)
anova_result

#Tukey test =>post hoc test(HSD.test)
out=HSD.test(ANAV,"Condition")
out

#The following line shows the stats for each group

out$groups




#If data do not follow a normal distribution, a Kruskal-Wallis test followed by a Dunn test (non-parametric) must be performed.


library(dunn.test)


# Load data
  thi <- read.csv2("HME1.csv")

#Perform the Kruskal-Wallis test
kruskal_result <- kruskal.test(Valeurs ~ Conditions, data = thi)
print(kruskal_result)

#Dunn's post-hoc test

dunn_results <- dunn.test(x = thi$Valeurs, g = thi$Conditions, list = TRUE, alpha = 0.05, method = "bonferroni")
pvalues <- dunn_results$P.adjusted
comparisons <- dunn_results$comparisons

# Get the letters as a Tukey test
#Create the comparison matrix
conditions <- unique(thi$Conditions)
comparison_matrix <- matrix(NA, nrow = length(conditions), ncol = length(conditions))
rownames(comparison_matrix) <- conditions
colnames(comparison_matrix) <- conditions

#Fill the matrix with p-values
# Use "-" as separator

for (i in 1:length(comparisons)) {
  groups <- unlist(strsplit(comparisons[i], " - ")) 
  

 # Do not fill in for identical comparisons
 
  if (length(groups) == 2 && all(groups %in% rownames(comparison_matrix))) {
    if (groups[1] != groups[2]) {  
      comparison_matrix[groups[1], groups[2]] <- pvalues[i]
      comparison_matrix[groups[2], groups[1]] <- pvalues[i]
    }
  } else {
    warning(paste("Groups not found or incorrect number of groups:", paste(groups, collapse = ", ")))
  }
}


# Using multcompLetters
# Load multcompLetters
library(multcomp)
if (any(is.na(comparison_matrix))) {
  stop("The comparison matrix contains missing values. Please check entries.")
} else {
  group_letters <- multcompLetters(comparison_matrix)$Letters
  print(group_letters)
}

